<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>EECS 221 Project | EECS221 Project Page</title>
    <meta name="generator" content="VuePress 1.9.10">
    
    <meta name="description" content="">
    
    <link rel="preload" href="/EECS221/assets/css/0.styles.d3980d3d.css" as="style"><link rel="preload" href="/EECS221/assets/js/app.1cd2037b.js" as="script"><link rel="preload" href="/EECS221/assets/js/2.e4a27e58.js" as="script"><link rel="preload" href="/EECS221/assets/js/1.85650cee.js" as="script"><link rel="preload" href="/EECS221/assets/js/22.4adcb313.js" as="script"><link rel="prefetch" href="/EECS221/assets/js/10.76fcfc08.js"><link rel="prefetch" href="/EECS221/assets/js/11.43d2f2e0.js"><link rel="prefetch" href="/EECS221/assets/js/12.c14fe52a.js"><link rel="prefetch" href="/EECS221/assets/js/13.8fda008b.js"><link rel="prefetch" href="/EECS221/assets/js/14.4b418a80.js"><link rel="prefetch" href="/EECS221/assets/js/15.ff0698b7.js"><link rel="prefetch" href="/EECS221/assets/js/16.63bdb408.js"><link rel="prefetch" href="/EECS221/assets/js/17.aac8b3bb.js"><link rel="prefetch" href="/EECS221/assets/js/18.7efbad74.js"><link rel="prefetch" href="/EECS221/assets/js/19.d8afd0ae.js"><link rel="prefetch" href="/EECS221/assets/js/20.10e47ab9.js"><link rel="prefetch" href="/EECS221/assets/js/21.ec9701e9.js"><link rel="prefetch" href="/EECS221/assets/js/3.6f6c78c1.js"><link rel="prefetch" href="/EECS221/assets/js/4.45665f8a.js"><link rel="prefetch" href="/EECS221/assets/js/5.7098d77a.js"><link rel="prefetch" href="/EECS221/assets/js/6.0c0a0f39.js"><link rel="prefetch" href="/EECS221/assets/js/7.6a854e57.js"><link rel="prefetch" href="/EECS221/assets/js/vendors~docsearch.e709020c.js">
    <link rel="stylesheet" href="/EECS221/assets/css/0.styles.d3980d3d.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/EECS221/" aria-current="page" class="home-link router-link-exact-active router-link-active"><!----> <span class="site-name">EECS221 Project Page</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"> <a href="https://github.com/gtz123456/EECS221" target="_blank" rel="noopener noreferrer" class="repo-link">
    Github Repo
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"> <a href="https://github.com/gtz123456/EECS221" target="_blank" rel="noopener noreferrer" class="repo-link">
    Github Repo
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="eecs-221-project"><a href="#eecs-221-project" class="header-anchor">#</a> EECS 221 Project</h1> <h2 id="team-members"><a href="#team-members" class="header-anchor">#</a> Team members</h2> <p><strong>Group 4 Members:</strong> Mengting Yang, Tianzhou Gao, Xuzhong Chen, Ang Li</p> <h2 id="abstract"><a href="#abstract" class="header-anchor">#</a> Abstract</h2> <p>In the evolving landscape of mixed reality (MR), ensuring user security and ease of access remains a significant challenge. Traditional authentication methods, like usernames and passwords, are cumbersome and less secure when applied to MR environments such as those encountered with Microsoft HoloLens. Furthermore, the HoloLens design does not support user-facing cameras, rendering facial recognition technologies infeasible for user identification. This limitation necessitates an innovative approach to authenticate users efficiently without compromising their privacy.
Moreover, the standard practice of processing data on external devices introduces additional vulnerabilities, exposing users to potential security breaches. Our goal is to revolutionize user identification by developing a seamless, real-time authentication process that operates entirely on-device, eliminating the need for external data transmission and reducing the risk of privacy violations.
Explore our solution to navigate the intersection of security and functionality, ensuring a secure and user-friendly experience in mixed reality environments.</p> <h2 id="problem-definition"><a href="#problem-definition" class="header-anchor">#</a> Problem Definition</h2> <ul><li><strong>Inefficient Login Mechanisms:</strong> Traditional login systems like usernames and passwords are impractical and insecure for mixed reality platforms.</li> <li><strong>Biometric Authentication Limitations:</strong> The design constraints of the HoloLens, which lack forward-facing cameras, rule out facial recognition for verifying user identity.</li> <li><strong>Vulnerability to Data Theft:</strong> Sending data to external systems for processing exposes users to increased risks of unauthorized access and data theft.</li> <li><strong>Integration Challenges:</strong> There is an essential need for an authentication system that integrates directly with the device, operates in real time, and maintains user privacy without external data transfers.</li></ul> <h2 id="challenges-and-approaches"><a href="#challenges-and-approaches" class="header-anchor">#</a> Challenges and Approaches</h2> <h3 id="challenges"><a href="#challenges" class="header-anchor">#</a> Challenges</h3> <ul><li><strong>Resource Limitations:</strong> The HoloLens is a resource-constrained device, making it challenging to run complex machine learning models for real-time authentication.</li> <li><strong>Privacy Concerns:</strong> Ensuring that user data remains private and is not exposed during the authentication process requires innovative solutions that avoid external data transmissions.</li> <li><strong>Usability:</strong> Maintaining a seamless and user-friendly authentication experience in a mixed reality environment without interrupting user interaction.</li> <li><strong>Accuracy:</strong> Developing a reliable authentication system that accurately identifies users based on limited and non-traditional biometric data like hand gestures and eye movements.</li></ul> <h3 id="approaches"><a href="#approaches" class="header-anchor">#</a> Approaches</h3> <ul><li><strong>Efficient Machine Learning Models:</strong> Exploring and implementing lightweight machine learning algorithms such as Random Forest, KNN, SVM, and XGBoost that can operate effectively on the HoloLens.</li> <li><strong>On-device Processing:</strong> All authentication processes are conducted locally on the HoloLens to avoid the need for external data transfers, enhancing privacy and security.</li> <li><strong>Background Authentication:</strong> Utilizing sensors to collect data unobtrusively in the background during regular user interactions with the MR environment, ensuring the authentication process is non-invasive.</li> <li><strong>Data Optimization:</strong> Adjusting the sample rates and data collection methods to balance between authentication accuracy and system performance, minimizing the impact on device resources and user experience.</li></ul> <h2 id="timeline"><a href="#timeline" class="header-anchor">#</a> Timeline:</h2> <table><thead><tr><th>Week</th> <th>Activity</th></tr></thead> <tbody><tr><td>Week 6</td> <td>Completion of Unity game design and commencement of data collection, followed by the midterm presentation.</td></tr> <tr><td>Week 7</td> <td>Continued data collection and analysis. Focus on machine learning model training and iterative optimization based on evaluation results.</td></tr> <tr><td>Week 8</td> <td>Comparative performance analysis of different machine learning algorithms to select the optimal one for deployment.</td></tr> <tr><td>Week 9</td> <td>Deployment of the selected machine learning model on HoloLens for real-time analysis.</td></tr> <tr><td>Week 10</td> <td>Comprehensive system testing, debugging, and revision of designs as needed to refine the application.</td></tr> <tr><td>Final Week</td> <td>Final presentation and submission, wrapping up the project documentation and outcomes.</td></tr></tbody></table> <h2 id="preliminary-design"><a href="#preliminary-design" class="header-anchor">#</a> Preliminary Design</h2> <p>The preliminary design of our project encompasses a comprehensive setup of hardware and software components, specifically tailored to enhance user authentication in mixed reality environments using the Microsoft HoloLens 2. Below is a detailed breakdown of the systems and methodologies involved:</p> <h3 id="hardware"><a href="#hardware" class="header-anchor">#</a> Hardware</h3> <ul><li><strong>Microsoft HoloLens 2</strong>: Chosen for its advanced mixed reality capabilities and sensor array. This headset is integral to our solution, providing the necessary hardware platform for deploying our authentication algorithms.</li></ul> <h3 id="software-components"><a href="#software-components" class="header-anchor">#</a> Software Components</h3> <ul><li><strong>Unity Engine</strong>: Employed to create a dynamic and interactive mixed reality environment where users can engage with virtual objects. Unity is pivotal for simulating real-world scenarios in which user identification needs to be tested and validated. Below is a demo video showing how our Unity game works:
<a href="https://www.youtube.com/watch?v=2dj5jzh1z3Y" title="Unity game" target="_blank" rel="noopener noreferrer"><img src="https://res.cloudinary.com/marcomontalbano/image/upload/v1715239779/video_to_markdown/images/youtube--2dj5jzh1z3Y-c05b58ac6eb4c4700831b2b3070cd403.jpg" alt="Unity game"><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><strong>Microsoft Mixed Reality Toolkit (MRTK)</strong>: A collection of scripts and components intended to accelerate the development of mixed reality applications. MRTK is utilized to streamline interactions and simplify the usage of device features like spatial awareness and hand tracking.</li> <li><strong>HoloLens 2 Sensor Streaming (hl2ss)</strong>: This component is crucial for accessing real-time sensor data from the HoloLens 2. It enables the capture of detailed biometric data, which is essential for our machine learning models to perform user identification. The example data is shown in the picture below:</li></ul> <p><img src="format.png" alt="data_format" title="Data format"></p> <h3 id="machine-learning-libraries"><a href="#machine-learning-libraries" class="header-anchor">#</a> Machine Learning Libraries</h3> <ul><li><strong>XGBoost</strong>: Selected for its performance in classification problems, particularly under constraints of computational resources, which is a common challenge in wearable technology.</li> <li><strong>SVM (Support Vector Machine)</strong>: Used for its effectiveness in high-dimensional spaces, which is typical for sensor data from HoloLens 2.</li> <li><strong>Random Forest</strong>: Chosen for its robustness in handling various types of data and its ability to run efficiently on limited datasets.</li> <li><strong>KNN (K-Nearest Neighbors)</strong>: Utilized for its simplicity and effectiveness in classification by comparing new data points with known data points.</li></ul> <h3 id="unity-game-design"><a href="#unity-game-design" class="header-anchor">#</a> Unity Game Design</h3> <p>A simple Unity MR game is designed for simulating user activity in general MR applications. A demo video is presented below:</p> <h3 id="data-collection-strategy"><a href="#data-collection-strategy" class="header-anchor">#</a> Data Collection Strategy</h3> <p>During interactions within the Unity-developed MR environment, data is captured passively from the user, focusing on non-intrusive metrics to ensure comfort and natural behavior:</p> <ul><li><strong>Hand Joint Data</strong>: Tracks the position and orientation of each joint in the user’s hands, providing a detailed profile of hand movements.</li> <li><strong>Eye Gaze Data</strong>: Monitors where the user is looking within the virtual environment, offering insights into user focus and intent.</li> <li><strong>Hand Pose Data</strong>: Collects comprehensive data on the positioning and movement of the user’s hands, which is pivotal for gestures recognition.</li></ul> <h3 id="data-storage-and-processing"><a href="#data-storage-and-processing" class="header-anchor">#</a> Data Storage and Processing</h3> <ul><li><strong>CSV Files</strong>: Sensor data, including positions and orientations, is logged into CSV files. This format facilitates easy manipulation and analysis, serving as the foundational dataset for training our machine learning models.</li> <li><strong>Local Processing</strong>: All data is processed on the device to ensure privacy and security, aligning with our goal to create a self-contained system that does not rely on external data processing.</li></ul> <p>This preliminary design is aimed at establishing a robust foundation for developing a secure and efficient user authentication system within mixed reality applications, addressing both technical and user-experience challenges.</p> <h2 id="references"><a href="#references" class="header-anchor">#</a> References:</h2> <h3 id="articles"><a href="#articles" class="header-anchor">#</a> Articles</h3> <ol><li>Cong Shi, Xiangyu Xu, Tianfang Zhang, Payton Walker, Yi Wu, Jian Liu, Nitesh Saxena, Yingying Chen, and Jiadi Yu. 2021. Face-Mic: inferring live speech and speaker identity via subtle facial dynamics captured by AR/VR motion sensors. In Proceedings of the 27th Annual International Conference on Mobile Computing and Networking (MobiCom '21). Association for Computing Machinery, New York, NY, USA, 478–490. https://doi.org/10.1145/3447993.3483272</li> <li>Jarin, I., Duan, Y., Trimananda, R., Cui, H., Elmalaki, S., &amp; Markopoulou, A. (2023). BehaVR: User Identification Based on VR Sensor Data. arXiv preprint arXiv:2308.07304.</li> <li>Miller, M. R., Herrera, F., Jun, H., Landay, J. A., &amp; Bailenson, J. N. (2020). Personal identifiability of user tracking data during observation of 360-degree VR video. Scientific Reports, 10(1), 17404.</li> <li>Vijayan, V.; Connolly, J.P.; Condell, J.; McKelvey, N.; Gardiner, P. Review of Wearable Devices and Data Collection Considerations for Connected Health. Sensors 2021, 21, 5589. https://doi.org/10.3390/s21165589</li></ol> <h3 id="sdk-and-libraries"><a href="#sdk-and-libraries" class="header-anchor">#</a> SDK and Libraries</h3> <ol><li>Windows Runtime API: JointPose Struct https://learn.microsoft.com/en-us/uwp/api/windows.perception.people.jointpose?view=winrt-22621</li> <li>MRTK: Extended eye tracking in native engine https://learn.microsoft.com/en-us/uwp/api/windows.perception.people.jointpose?view=winrt-22621</li> <li>hl2ss: https://github.com/jdibenes/hl2ss</li></ol></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/EECS221/assets/js/app.1cd2037b.js" defer></script><script src="/EECS221/assets/js/2.e4a27e58.js" defer></script><script src="/EECS221/assets/js/1.85650cee.js" defer></script><script src="/EECS221/assets/js/22.4adcb313.js" defer></script>
  </body>
</html>
